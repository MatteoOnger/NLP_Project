{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "eoTZ_C3yIyiS",
        "XUr-LS1WJMJB",
        "saKBkgXAosYm",
        "zjZPF6zjRRwk",
        "12eeFNEvWvYp",
        "LYuhiWCwZH2E",
        "cYkUiKJrZMSP",
        "YgjO6arkZSw1",
        "wLk_G-aXZVFu",
        "fjLuBTyjkBaG"
      ],
      "authorship_tag": "ABX9TyNtPZ+2vULJYQigPPnsBdhR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Project: Enron Email Dataset**\n",
        "\n",
        "*   **Author:** Matteo Onger\n",
        "*   **Date:** December 2024"
      ],
      "metadata": {
        "id": "XvcVpru0tpu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset documentation**:\n",
        "*   [Enron email dataset](https://www.kaggle.com/datasets/wcukierski/enron-email-dataset)\n",
        "\n",
        "**Extra documentation**:\n",
        "*   [POS](https://universaldependencies.org/u/pos/)\n",
        "*   [SpaCy - English](https://spacy.io/models/en#en_core_web_sm)\n",
        "\n",
        "**Notes**:\n",
        "*   Only basic parameters can be changed via the forms."
      ],
      "metadata": {
        "id": "OqwGP1zJtujv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- LIBRARIES ----\n",
        "import email\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "from enum import Enum\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import normalize\n",
        "from spacy.language import Language\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Any, Literal\n",
        "\n",
        "\n",
        "# init spacy\n",
        "flag_gpu = spacy.prefer_gpu()\n",
        "print(f\"GPU available:{flag_gpu}\")\n",
        "\n",
        "# init tpqdm\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "WSNfJzTedW2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c41cd9-31e4-49d2-a05a-b265c180fdb8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available:False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- FUNCTIONS ----\n",
        "def extract_fields(msg :email.message, fields :list[str]|None=None) -> dict[str, str]:\n",
        "    \"\"\"\n",
        "    This function extracts the required fields from the given email message.\n",
        "    If field is ``None``, all the available ones are returned.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    msg : email.message\n",
        "        Email message.\n",
        "    fields : list | None, optional\n",
        "        The fields to extract, by default ``None``.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    :dict[str, str]\n",
        "        A dictionary ``{field_name:field_value}`` containing the fields extracted is returned.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - If  ``fields`` is ``None``, all the available fields are extracted.\n",
        "    - Fields must be in lower case.\n",
        "    \"\"\"\n",
        "    res = dict()\n",
        "\n",
        "    if fields is None:\n",
        "        res = {str.lower(key):msg[key] for key in msg.keys()}\n",
        "    else:\n",
        "        res = {str.lower(key):msg[key] for key in msg.keys() if str.lower(key) in fields}\n",
        "\n",
        "    if fields is None or \"content\" in fields:\n",
        "        parts = list()\n",
        "        for part in msg.walk():\n",
        "            if part.get_content_type() == 'text/plain':\n",
        "                parts.append(part.get_payload())\n",
        "        res[\"content\"] = \"\".join(parts)\n",
        "    return res\n",
        "\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "    \"\"\"\n",
        "    Returns ``text`` as it is.\n",
        "    \"\"\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove(lt :list, value :Any) -> list:\n",
        "    \"\"\"\n",
        "    Returns a copy of the given list without the given value.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lt : list\n",
        "        List to modify.\n",
        "    value : Any\n",
        "        Value to remove.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    :list\n",
        "        A new list without ``value``.\n",
        "    \"\"\"\n",
        "    lt_copy = list(lt)\n",
        "    lt_copy.remove(value)\n",
        "    return lt_copy\n",
        "\n",
        "\n",
        "def split_email_addresses(addresses :str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Extracts the email addresses contained in the provided string.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    addresses : str\n",
        "        String containing email addresses.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    :list[str]\n",
        "        List of strings that are the email addresses found.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - If ``addresses`` is not a string, an empty list is returned.\n",
        "    \"\"\"\n",
        "    if isinstance(addresses, str):\n",
        "        addrs = re.findall(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', addresses)\n",
        "    else:\n",
        "        addrs = list()\n",
        "    return addrs"
      ],
      "metadata": {
        "id": "cGFhHQYDGqS2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- CLASSES ----\n",
        "class Preprocessor():\n",
        "    \"\"\"\n",
        "    This class implements a simple preprocessor to split documents into lists of tokens.\n",
        "    It is mainly based on spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    class CREIL(Enum):\n",
        "        \"\"\"\n",
        "        Common regex of invalid lines.\n",
        "        \"\"\"\n",
        "        HEADER = r\"^\\s*(?:-{5,}|.*-{5,}|from:|to:|cc:|ccn:|sent by:|subject:).*$\"\n",
        "        \"\"\"\n",
        "        Removes (forwarded, etc.) email header and blank lines.\n",
        "        \"\"\"\n",
        "\n",
        "    class CREIT(Enum):\n",
        "        \"\"\"\n",
        "        Common regex of invalid tokens.\n",
        "        \"\"\"\n",
        "        ALPHA2 = r\"^(?:.*[^a-z]{1,}.*|.)$\"\n",
        "        \"\"\"\n",
        "        Only tokens of two or more characters consisting only of alphabetic characters are retained.\n",
        "        \"\"\"\n",
        "        ALPHA_2 = r\"^(?:.*[^a-z_-]{1,}.*|.)$\"\n",
        "        \"\"\"\n",
        "        Only tokens of two or more characters consisting only of alphabetic characters, and high or low dashes, are retained.\n",
        "        \"\"\"\n",
        "        ALPHANUM = r\".*(?:\\W|_).*\"\n",
        "        \"\"\"\n",
        "        Removes tokens that contain non-alphanumeric characters, '_' included.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    @Language.component(\"lower_case_lemmas\")\n",
        "    def _lower_case_lemmas(doc :spacy.tokens.Doc) -> spacy.tokens.Doc:\n",
        "        \"\"\"\n",
        "        Changes the capitalization of the lemmas to lowercase.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        doc : spacy.tokens.Doc\n",
        "            Doc to modify.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :spacy.tokens.Doc\n",
        "            Doc modified.\n",
        "        \"\"\"\n",
        "        for token in doc :\n",
        "            token.lemma_ = token.lemma_.lower()\n",
        "        return doc\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        case_sensitive :bool=True,\n",
        "        lemmatize :bool=False,\n",
        "        noun_chunks :bool=False,\n",
        "        keep_stopwords :bool=True,\n",
        "        extend_stopwords :set[str]|None=None,\n",
        "        regex_flags :int=0,\n",
        "        regex_invalid_line :'str|Preprocessor.CREIL|None'=None,\n",
        "        regex_invalid_tokens :'str|Preprocessor.CREIT|None'=None,\n",
        "        ent_to_keep :set[str]|None=None,\n",
        "        ent_to_rm :set[str]|None=None,\n",
        "        pos_to_keep :set[str]|None=None,\n",
        "        pos_to_rm :set[str]|None=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        case_sensitive : bool, optional\n",
        "            If ``False``, all returned tokens are lowercase, by default ``True``.\n",
        "        lemmatize : bool, optional\n",
        "            If ``True``, tokens are lemmatized, by default ``False``.\n",
        "        noun_chunks : bool, optional\n",
        "            If ``True``, chunk nouns are merged, by default ``False``.\n",
        "        keep_stopwords : bool, optional\n",
        "            If ``True``, stop words are kept, by default ``True``.\n",
        "        extend_stopwords : set[str] | None, optional\n",
        "            List of stop words to add, by default ``None``.\n",
        "        regex_flags : int, optional\n",
        "            Flags of the regular expressions, by default ``0``.\n",
        "        regex_invalid_line : str | Preprocessor.CREIL | None, optional\n",
        "            Regex to mark lines that must be excluded, by default ``None``.\n",
        "        regex_invalid_tokens : str | Preprocessor.CREIT | None, optional\n",
        "            Regex to mark tokens that must to be excluded, by default ``None``.\n",
        "        ent_to_keep : set[str] | None, optional\n",
        "            Only tokens marked with one of these entity tags are kept,\n",
        "            by default all tokens are retained.\n",
        "        ent_to_rm : set[str] | None, optional\n",
        "            Tokens marked with one of these entity tags are removed,\n",
        "            by default all tokens are retained.\n",
        "        pos_to_keep : set[str] | None, optional\n",
        "            Only tokens marked with one of these POS tags are kept,\n",
        "            by default all tokens are retained.\n",
        "        pos_to_rm : set[str] | None, optional\n",
        "            Tokens marked with one of these POS tags are removed,\n",
        "            by default all tokens are retained.\n",
        "        \"\"\"\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "        # update nlp piepline\n",
        "        if not lemmatize and not noun_chunks:\n",
        "            self.nlp.remove_pipe(\"lemmatizer\")\n",
        "            if ent_to_keep is None and ent_to_rm is None and pos_to_keep is None and pos_to_rm is None:\n",
        "                self.nlp.remove_pipe(\"tagger\")\n",
        "                self.nlp.remove_pipe(\"parser\")\n",
        "                self.nlp.remove_pipe(\"attribute_ruler\")\n",
        "                self.nlp.remove_pipe(\"ner\")\n",
        "        if noun_chunks:\n",
        "            self.nlp.add_pipe(\"merge_noun_chunks\")\n",
        "        if lemmatize and not case_sensitive:\n",
        "            self.nlp.add_pipe(\"lower_case_lemmas\", name=\"lower_case_lemmas\")\n",
        "\n",
        "        # standard regex\n",
        "        self.regex_empty_line = r\"^\\s*$\"\n",
        "        self.regex_endline_1 = r\"(=20\\n)\"\n",
        "        self.regex_endline_2 = r\"(=\\n)\"\n",
        "\n",
        "        self.pattern_empty_line = re.compile(self.regex_empty_line, regex_flags)\n",
        "        self.pattern_endline_1 = re.compile(self.regex_endline_1, regex_flags)\n",
        "        self.pattern_endline_2 = re.compile(self.regex_endline_2, regex_flags)\n",
        "\n",
        "        # save fields\n",
        "        self.case_sensitive = case_sensitive\n",
        "        self.noun_chunks = noun_chunks\n",
        "        self.lemmatize = lemmatize\n",
        "        self.keep_stopwords = keep_stopwords\n",
        "        self.extend_stopwords = extend_stopwords\n",
        "        self.regex_flags = regex_flags\n",
        "        self.regex_invalid_line = regex_invalid_line.value if isinstance(regex_invalid_line, Preprocessor.CREIL) else regex_invalid_line\n",
        "        self.regex_invalid_tokens = regex_invalid_tokens.value if isinstance(regex_invalid_tokens,  Preprocessor.CREIT) else regex_invalid_tokens\n",
        "        self.ent_to_keep = ent_to_keep\n",
        "        self.ent_to_rm = ent_to_rm\n",
        "        self.pos_to_keep = pos_to_keep\n",
        "        self.pos_to_rm = pos_to_rm\n",
        "\n",
        "        # assemble the condition that tokens/lines must satisfy\n",
        "        self.conditions = list()\n",
        "        if not keep_stopwords:\n",
        "            self.conditions.append(lambda x: not x.is_stop)\n",
        "            if extend_stopwords is not None:\n",
        "                if lemmatize:\n",
        "                    self.conditions.append(lambda x: x.lemma_ not in extend_stopwords)\n",
        "                else:\n",
        "                    self.conditions.append(lambda x: x.text not in extend_stopwords if case_sensitive else x.text.lower() not in extend_stopwords)\n",
        "        if ent_to_keep is not None:\n",
        "            self.conditions.append(lambda x: x.ent_type_ in ent_to_keep)\n",
        "        if ent_to_rm is not None:\n",
        "            self.conditions.append(lambda x: x.ent_type_ not in ent_to_rm)\n",
        "        if pos_to_keep is not None:\n",
        "            self.conditions.append(lambda x: x.pos_ in pos_to_keep)\n",
        "        if pos_to_rm is not None:\n",
        "            self.conditions.append(lambda x: x.pos_ in pos_to_rm)\n",
        "\n",
        "        if regex_invalid_line is not None:\n",
        "            self.pattern_invalid_line = re.compile(self.regex_invalid_line, regex_flags)\n",
        "        if regex_invalid_tokens is not None:\n",
        "            self.pattern_invalid_tokens = re.compile(self.regex_invalid_tokens, regex_flags)\n",
        "            self.conditions.append(lambda x: not self.pattern_invalid_tokens.match(x.text))\n",
        "        return\n",
        "\n",
        "\n",
        "    def preprocess(self, document :str) -> list[str]:\n",
        "        \"\"\"\n",
        "        Applies the preprocessing procedure to the given document.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        document : str\n",
        "            Document to process.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :list[str]\n",
        "            List of tokens representing the document.\n",
        "        \"\"\"\n",
        "        # MIME protocol for base64, the max line length in the encoded data is 76 characters\n",
        "        # an '=' sign at the end of an encoded line is used to tell the decoder that the line is to be continued,\n",
        "        # while an '=20' sign is used to tell the line ends\n",
        "        document = self.pattern_endline_1.sub(\"\\n\", document)\n",
        "        document = self.pattern_endline_2.sub(\"\", document)\n",
        "\n",
        "        # read the document line by line and remove the invalid ones that are:\n",
        "        # - blank lines\n",
        "        # - lines that match the given regex <self.regex_invalid_line>\n",
        "        # - lines preceded by invalid non-white lines\n",
        "        prev = True\n",
        "        filtered_document = list()\n",
        "        for line in document.splitlines():\n",
        "            if self.pattern_empty_line.match(line):\n",
        "                prev = True\n",
        "            elif prev and (self.regex_invalid_line is None or not self.pattern_invalid_line.match(line)):\n",
        "                filtered_document.append(line)\n",
        "            else:\n",
        "                prev = False\n",
        "\n",
        "        # replace all withespaces with a single whitespace\n",
        "        document = re.sub(r\"\\s{1,}\", \" \", \"\\n\".join(filtered_document))\n",
        "\n",
        "        # tokenize, etc. using spaCy\n",
        "        doc = self.nlp(document)\n",
        "        if self.lemmatize:\n",
        "            tokens = [tk.lemma_ for tk in doc if all(f(tk) for f in self.conditions)]\n",
        "        else:\n",
        "            if self.case_sensitive:\n",
        "                tokens = [tk.text for tk in doc if all(f(tk) for f in self.conditions)]\n",
        "            else:\n",
        "                tokens = [tk.text.lower() for tk in doc if all(f(tk) for f in self.conditions)]\n",
        "        return tokens\n",
        "\n",
        "\n",
        "\n",
        "class KMeansHelper():\n",
        "    \"\"\"\n",
        "    This class is based on the module ``sklearn.cluster.KMeans`` and\n",
        "    provides some heuristicher to estimate the best number of clusters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        init :str=\"k-means++\",\n",
        "        n_init :int|str=\"auto\",\n",
        "        max_iter :int=300,\n",
        "        random_state :int|None=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        init : str, optional\n",
        "            Method for initialization, by default ``'k-means++'``:\n",
        "            - ``'k-means++'``: selects initial cluster centroids using sampling based on an empirical probability distribution\n",
        "            of the points' contribution to the overall inertia. This technique speeds up convergence.\n",
        "            The algorithm implemented is greedy k-means++. It differs from the vanilla k-means++ by making several trials\n",
        "            at each sampling step and choosing the best centroid among them.\n",
        "            - ``'random'``: choose n_clusters observations at random from data for the initial centroids.\n",
        "        n_init : str, optional\n",
        "            Number of times the k-means algorithm is run with different centroid seeds.\n",
        "            By default ``n_init='auto'``, the number of runs depends on the value of init:\n",
        "            10 if using ``init='random'``; 1 if using ``init='k-means++'``.\n",
        "        max_iter : int, optional\n",
        "            Maximum number of iterations of the k-means algorithm for a single run, by default ``300``.\n",
        "        random_state : int | None, optional\n",
        "            Determines random number generation for centroid initialization. Use an int to make the randomness deterministic,\n",
        "            by default ``None``.\n",
        "        \"\"\"\n",
        "        self.init = init\n",
        "        self.n_init = n_init\n",
        "        self.max_iter = max_iter\n",
        "        self.random_state = random_state\n",
        "        return\n",
        "\n",
        "\n",
        "    def elbow_method(self, X :np.ndarray, min_K :int, max_K :int) -> dict[int, int]:\n",
        "        \"\"\"\n",
        "        Returns and draws the inertia as the number of clusters changes.\n",
        "        This heuristic is used to determine the number of clusters in a data set ``X``.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray\n",
        "            New data to transform.\n",
        "        min_K : int\n",
        "            Minimum number of clusters.\n",
        "        max_K : int\n",
        "            maximum number of clusters.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :dict[int, int]\n",
        "            A dictionary containing the SSE for every possible number of clusters\n",
        "            between ``min_K`` and ``max_K``.\n",
        "        \"\"\"\n",
        "        sse = {}\n",
        "        for k in range(min_K, max_K):\n",
        "            kmeans = KMeans(n_clusters=k, init=self.init, n_init=self.n_init, max_iter=self.max_iter, random_state=self.random_state).fit(X)\n",
        "            sse[k] = kmeans.inertia_\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(list(sse.keys()), list(sse.values()), marker='*')\n",
        "        plt.title(\"Elbow method\")\n",
        "        plt.xlabel(\"Number of cluster (K)\")\n",
        "        plt.ylabel(\"SSE\")\n",
        "        plt.show()\n",
        "        return sse\n",
        "\n",
        "\n",
        "    def fit_predict(self, X :np.ndarray, K :int) -> tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Compute cluster centers and predict cluster index for each sample.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray\n",
        "            New data to transform.\n",
        "        K : int\n",
        "            Number of clusters.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :tuple[np.ndarray, np.ndarray]\n",
        "            Index of the cluster each sample belongs to and the centroids.\n",
        "        \"\"\"\n",
        "        kmeans = KMeans(n_clusters=K, init=self.init, n_init=self.n_init, max_iter=self.max_iter, random_state=self.random_state)\n",
        "        clusters = kmeans.fit_predict(X)\n",
        "        centroids = kmeans.cluster_centers_\n",
        "        return clusters, centroids\n",
        "\n",
        "\n",
        "    def silhouette_score(self, X :np.ndarray, min_K :int, max_K :int) -> dict[int, int]:\n",
        "        \"\"\"\n",
        "        Returns the silhouette score [-1, 1] as the number of clusters changes.\n",
        "        This heuristic is used to determine the number of clusters in the dataset ``X``.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray\n",
        "            New data to transform.\n",
        "        min_K : int\n",
        "            Minimum number of clusters.\n",
        "        max_K : int\n",
        "            maximum number of clusters.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :dict[int, int]\n",
        "            A dictionary containing the silhouette score for every possible number of clusters\n",
        "            between ``min_K`` and ``max_K``.\n",
        "        \"\"\"\n",
        "        sc = {}\n",
        "        print(\"Number of clusters:\")\n",
        "        for k in range(min_K, max_K):\n",
        "            kmeans = KMeans(n_clusters=k, init=self.init, n_init=self.n_init, max_iter=self.max_iter, random_state=self.random_state).fit(X)\n",
        "            sc[k] = silhouette_score(X, kmeans.labels_, metric='euclidean')\n",
        "\n",
        "            s = f\" - K:{k} => Silhouette coeff.: {sc[k]}\"\n",
        "            if (k != min_K) and (sc[k] < sc[k-1]):\n",
        "                s += \" *\"\n",
        "            print(s)\n",
        "        return sc\n",
        "\n",
        "\n",
        "\n",
        "class myDoc2vec():\n",
        "    \"\"\"\n",
        "    This class maps a document to a vector using a weighted average of the vectors\n",
        "    associated with the tokens that constitute the document under consideration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "        w2v :gensim.models.KeyedVectors,\n",
        "        tfidf_vectorizer :TfidfVectorizer,\n",
        "        exp_a :float=1.0,\n",
        "        exp_b :float=1.0,\n",
        "        eps :float=0.0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        w2v : gensim.models.KeyedVectors\n",
        "            Word vectors to use.\n",
        "        tfidf_vectorizer : TfidfVectorizer\n",
        "            Vectorizer to use to compute the TF-IDF score.\n",
        "        exp_a : float, optional\n",
        "            Exponent of the Tf-Idf term, by default ``1.0``.\n",
        "        exp_b : float, optional\n",
        "            Exponent of the similarity term, by default ``1.0``.\n",
        "        eps : float, optional\n",
        "            Threshold, by default ``0.0``.\n",
        "        \"\"\"\n",
        "        self.w2v = w2v\n",
        "        self.tfidf_vectorizer = tfidf_vectorizer\n",
        "        self.exp_a = exp_a\n",
        "        self.exp_b = exp_b\n",
        "        self.eps = eps\n",
        "        return\n",
        "\n",
        "\n",
        "    def transform(self, tokenized_corpus :list[list[str]], save :bool=False) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        For each document in the corpus, the function computes a vector representing it.\n",
        "        The vectors produced lie in the same space defined by the word vectors provided\n",
        "        during the object initialization.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        tokenized_corpus : list[list[str]]\n",
        "            Corpus, already tokenized, to be analyzed.\n",
        "        save : bool, optional\n",
        "            For debugging, if ``True``,\n",
        "            the weights of each token in each document are saved in ``self._weights``, by default ``False``.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :np.ndarray\n",
        "            A vector for each document.\n",
        "        \"\"\"\n",
        "        corpus_size = len(tokenized_corpus)\n",
        "        weights_corpus = self.get_weights(tokenized_corpus)\n",
        "\n",
        "        if save:\n",
        "            self._weights = weights_corpus\n",
        "        else:\n",
        "            self._weights = None\n",
        "\n",
        "        res = np.zeros((corpus_size, self.w2v.vector_size))\n",
        "        for i in range(corpus_size):\n",
        "            # list of unique tokens in i-th doc\n",
        "            # duplicated tokens are considered only once\n",
        "            unique_tokens = list(weights_corpus[i].keys())\n",
        "\n",
        "            # weight of each token\n",
        "            weights = list(weights_corpus[i].values())\n",
        "\n",
        "            # weighted mean\n",
        "            if len(unique_tokens) != 0:\n",
        "                res[i] = self.w2v.get_mean_vector(unique_tokens, weights=weights)\n",
        "        return res\n",
        "\n",
        "\n",
        "    def get_weights(self, tokenized_corpus :list[list[str]]) -> list[dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Computes the weight of each token in each document.\n",
        "        The weight of the token tk in document doc is computed as follows:\n",
        "        weight := tfidf(tk, doc)**exp_a * average similarity(tk, other tokens in doc)**exp_b;\n",
        "        if weight < eps, weight := 0.\n",
        "        The weights of a document are then normalised to sum to one.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        tokenized_corpus : list[list[str]]\n",
        "            Corpus, already tokenized, to be analyzed.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :list[dict[str, float]]\n",
        "            The weight of each token in each document.\n",
        "        \"\"\"\n",
        "        corpus_size = len(tokenized_corpus)\n",
        "\n",
        "        tfidf_corpus = self.tfidf_vectorizer.fit_transform(tokenized_corpus)\n",
        "        tf_corpus = normalize(tfidf_corpus / self.tfidf_vectorizer.idf_, norm=self.tfidf_vectorizer.norm)\n",
        "\n",
        "        weights_corpus = list()\n",
        "        for i in range(corpus_size):\n",
        "            # term frequency of tokens in i-th document\n",
        "            tf_doc = tf_corpus[i].data\n",
        "\n",
        "            # list of unique tokens in i-th document and their position in the tfidf matrix\n",
        "            # duplicated tokens are considered only once\n",
        "            idx_unique_tokens = tf_corpus[i].nonzero()[1]\n",
        "            unique_tokens = list(\n",
        "                self.tfidf_vectorizer.get_feature_names_out()[idx_unique_tokens]\n",
        "            )\n",
        "\n",
        "            # compute the weight of each token in i-th document\n",
        "            if len(unique_tokens) == 0:\n",
        "                weights_corpus.append({})\n",
        "            elif len(unique_tokens) == 1:\n",
        "                weights_corpus.append({unique_tokens[0]:1.0})\n",
        "            else:\n",
        "                weights = np.zeros(len(unique_tokens))\n",
        "\n",
        "                for j, (idx, tk) in enumerate(zip(idx_unique_tokens, unique_tokens)):\n",
        "                    weights[j] = (\n",
        "                        tfidf_corpus[i, idx]**self.exp_a\n",
        "                    ) * (\n",
        "                        np.average(1 - self.w2v.distances(tk, remove(unique_tokens, tk)) / 2, weights=np.delete(tf_doc, j))**self.exp_b\n",
        "                    )\n",
        "\n",
        "                    if weights[j] < self.eps:\n",
        "                        weights[j] = 0.0\n",
        "\n",
        "                weights_corpus.append(\n",
        "                    dict(zip(unique_tokens, normalize([weights], norm=\"l1\")[0]))\n",
        "                )\n",
        "        return weights_corpus\n",
        "\n",
        "\n",
        "    def get_most_similar(self, tokenized_corpus :list[list[str]], topn :int=10) -> list[list[tuple[str, float]]]:\n",
        "        \"\"\"\n",
        "        Equivalent to calling the function ``self.w2v.most_similar`` on the vectors\n",
        "        returned by ``self.transform``.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        tokenized_corpus : list[list[str]]\n",
        "            Corpus, already tokenized, to be analyzed.\n",
        "        topn : int, optional\n",
        "            Number of most similar keys to return, by default ``10``.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :list[list[tuple[str, float]]]\n",
        "            The ``top-n`` most similar keys found for each document.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        - Finding the most similar keys for each vector is a slow operation;\n",
        "        if you are only interested in a few documents, it is better to call ``self.transform``\n",
        "        and then call ``self.w2v.most_similar`` only on the documents of interest.\n",
        "        \"\"\"\n",
        "        corpus_size = len(tokenized_corpus)\n",
        "\n",
        "        # weighted mean vectors\n",
        "        wm_vectors = self.transform(tokenized_corpus)\n",
        "\n",
        "        res = list()\n",
        "        for i in range(corpus_size):\n",
        "            res.append(\n",
        "                self.w2v.most_similar(wm_vectors[i], topn=topn)\n",
        "            )\n",
        "        return res"
      ],
      "metadata": {
        "id": "k-1hpTHhOCtS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "k42Mkiv2a_FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- DATASET ----\n",
        "# download latest version\n",
        "path = kagglehub.dataset_download(\"wcukierski/enron-email-dataset\")\n",
        "print(f\"Path to dataset files: {path}\")\n",
        "\n",
        "# read the dataset\n",
        "df = pd.read_csv(path + \"/emails.csv\")\n",
        "\n",
        "# from string to email.message\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: email.message_from_string(x))\n",
        "\n",
        "# extract main fields\n",
        "df = df.join(\n",
        "    df.apply(\n",
        "        lambda row: extract_fields(row.message, [\"message-id\", \"date\", \"from\", \"to\", \"subject\", \"content\"]),\n",
        "        axis='columns',\n",
        "        result_type='expand'\n",
        "    )\n",
        ")\n",
        "\n",
        "# drop unused column and duplicated messages\n",
        "df.drop(columns=[\"message\"], inplace=True)\n",
        "df.drop_duplicates([\"content\"], keep=\"first\", inplace=True)\n",
        "\n",
        "# split addresses into a list of strings\n",
        "df[\"from\"] = df[\"from\"].apply(\n",
        "    lambda x: split_email_addresses(x),\n",
        ")\n",
        "df[\"to\"] = df[\"to\"].apply(\n",
        "    lambda x: split_email_addresses(x),\n",
        ")"
      ],
      "metadata": {
        "id": "_SOiq5ihtsXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content's ..."
      ],
      "metadata": {
        "id": "lRUjILVZFs2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Corpus"
      ],
      "metadata": {
        "id": "yuogXTnXI-WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Minimum and maximum length (in characters) of emails to be analyzed:\n",
        "min_len =  1_000  # @param {type: \"number\"}\n",
        "max_len = 20_000  # @param {type: \"number\"}\n",
        "# @markdown ---\n",
        "\n",
        "corpus = df[df[\"content\"].apply(lambda x: len(x)).between(min_len, max_len)]\n",
        "\n",
        "display(corpus)\n",
        "print(f\"\\nRandom exemple =>\\n{corpus.sample(1).iloc[0, 6]}\")"
      ],
      "metadata": {
        "id": "i9pHmXixNa88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "0T6-0idLJJSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp = Preprocessor(\n",
        "    case_sensitive = False,\n",
        "    lemmatize = True,\n",
        "    noun_chunks = False,\n",
        "    keep_stopwords = False,\n",
        "    extend_stopwords = {\"alot\", \"cell\", \"email\", \"fax\", \"fyi\", \"mail\", \"hereto\", \"link\", \"lot\",\n",
        "                        \"number\", \"pm\", \"recipient\", \"sender\", \"thank\", \"thanks\", \"thereto\", \"thing\", \"time\"},\n",
        "    regex_flags = re.IGNORECASE,\n",
        "    regex_invalid_line = Preprocessor.CREIL.HEADER,\n",
        "    regex_invalid_tokens = Preprocessor.CREIT.ALPHA2,\n",
        "    pos_to_keep = {\"NOUN\"},\n",
        "    ent_to_rm = {\"DATE\", \"TIME\"}\n",
        ")\n",
        "\n",
        "tokenized_corpus = list(corpus[\"content\"].progress_apply(pp.preprocess))"
      ],
      "metadata": {
        "id": "xvyP3_QzXeu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random example\n",
        "idx = np.random.randint(0, len(tokenized_corpus))\n",
        "\n",
        "print(f\"Random exemple [iloc:{idx}, loc:{corpus.index[idx]}] =>\\n{tokenized_corpus[idx]}\")"
      ],
      "metadata": {
        "id": "cL07YR7-xFUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "eoTZ_C3yIyiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Skip tokens that do not have a relative document frequency in this range:\n",
        "min_df = 0.010  # @param {type: \"number\"}\n",
        "max_df = 1.000  # @param {type: \"number\"}\n",
        "# @markdown ---\n",
        "\n",
        "tfidf_vectorizer =  TfidfVectorizer(tokenizer=identity_tokenizer, token_pattern=None, lowercase=False, min_df=min_df, max_df=max_df)\n",
        "tfidf_corpus = tfidf_vectorizer.fit_transform(tokenized_corpus)\n",
        "\n",
        "pd.DataFrame(tfidf_corpus.toarray(), index=corpus.index, columns=tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "IXcnSURMXWA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K-Means"
      ],
      "metadata": {
        "id": "XUr-LS1WJMJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use elbow method and silhouette score to estimate the number of clusters\n",
        "kmeans = KMeansHelper()\n",
        "\n",
        "_ = kmeans.elbow_method(tfidf_corpus, 2, 12)\n",
        "_ = kmeans.silhouette_score(tfidf_corpus, 2, 12)"
      ],
      "metadata": {
        "id": "R-50ewHJ0RTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of clusters\n",
        "K = 6\n",
        "# number of tokens with the highest tf-idf score to show per cluster\n",
        "W = 5\n",
        "\n",
        "clusters, centroids = kmeans.fit_predict(tfidf_corpus, K)\n",
        "\n",
        "# for each cluster, sort the token indexes according to their tf-idf score (descending order)\n",
        "argsort_tokens = (np.argsort(centroids, axis=1)[:, ::-1])\n",
        "# find the index of W tokens with the highest tf-idf score that are not among\n",
        "# W tokens with the highest tf-idf score in other clusters\n",
        "unique_main_tk_idxs = np.full((K, W), -1)\n",
        "\n",
        "for k in range(K):\n",
        "    counter = 0\n",
        "    for tk_idx in argsort_tokens[k]:\n",
        "        if tk_idx not in np.delete(argsort_tokens[:, :W], k, axis=0) and tk_idx not in unique_main_tk_idxs:\n",
        "            unique_main_tk_idxs[k, counter] = tk_idx\n",
        "            counter += 1\n",
        "        if counter == W:\n",
        "            break\n",
        "\n",
        "    main_tokens = tfidf_vectorizer.get_feature_names_out()[argsort_tokens[k, :W]]\n",
        "    unique_main_tokens = tfidf_vectorizer.get_feature_names_out()[unique_main_tk_idxs[k]]\n",
        "    idx_docs_in_cluster = np.flatnonzero(clusters == k)\n",
        "\n",
        "    print(f\"Cluster {k} =>\")\n",
        "    print(f\" - size: {len(idx_docs_in_cluster)}\")\n",
        "    print(f\" - main tokens: {main_tokens}\")\n",
        "    print(f\" - unique main tokens: {unique_main_tokens}\")\n",
        "    print(f\" - random samples: {np.random.choice(idx_docs_in_cluster, 3)}\")\n",
        "print()\n",
        "\n",
        "# documents and centroids as points in a n-dimensional space\n",
        "points_nD = np.concatenate((centroids, tfidf_corpus.toarray()))\n",
        "\n",
        "# t-SNE to reduce the number of dimensions\n",
        "tsne = TSNE(n_components=2, max_iter=1000, perplexity=25)\n",
        "points_2D = tsne.fit_transform(points_nD)\n",
        "\n",
        "# plot the clusters with their centroids\n",
        "fig, ax = plt.subplots()\n",
        "scatter_docs = ax.scatter(points_2D[K:, 0], points_2D[K:, 1], c=clusters, alpha=0.5)\n",
        "scatter_cens = ax.scatter(points_2D[:K, 0], points_2D[:K, 1], c=[\"red\"], alpha=1.0, marker='^', label=\"Centroids\")\n",
        "legend_docs = ax.legend(*scatter_docs.legend_elements(), loc=\"lower right\", title=\"Clusters\")\n",
        "legend_cens = ax.legend(handles=[scatter_cens], loc=\"upper left\")\n",
        "ax.add_artist(legend_docs)\n",
        "ax.add_artist(legend_cens)\n",
        "plt.title(\"t-SNE: KMeans with TF-IDF vectors\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T9j3BdMOOO3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA"
      ],
      "metadata": {
        "id": "saKBkgXAosYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Number of topics:\n",
        "T = 10  # @param {type: \"number\"}\n",
        "# @markdown ---\n",
        "\n",
        "# word to id\n",
        "id2word = gensim.corpora.Dictionary(tokenized_corpus)\n",
        "\n",
        "# term frequency of tokens in i-th document\n",
        "tf_corpus = [id2word.doc2bow(tk) for tk in tokenized_corpus]\n",
        "\n",
        "lda = gensim.models.LdaMulticore(corpus=tf_corpus, id2word=id2word, num_topics=T, passes=10)\n",
        "for t in range(T):\n",
        "    print(f\"Topic {t} =>\")\n",
        "    print(lda.print_topic(t, topn=5))"
      ],
      "metadata": {
        "id": "38FlHEo_8GMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic"
      ],
      "metadata": {
        "id": "zjZPF6zjRRwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download libraries\n",
        "!pip install bertopic\n",
        "!pip install bertopic[gensim]"
      ],
      "metadata": {
        "id": "jrNPmusQRdHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "model = BERTopic(nr_topics=10, verbose=True)\n",
        "topics, probabilities = model.fit_transform(corpus[\"content\"].to_list())"
      ],
      "metadata": {
        "id": "thYPOrZSRhz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get most frequent topics\n",
        "topic_freq = model.get_topic_freq()\n",
        "topic_freq.head(10)"
      ],
      "metadata": {
        "id": "v66FSPULRsv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get most impo words for of the most frquent topic\n",
        "model.get_topic(topic_freq.index[0])"
      ],
      "metadata": {
        "id": "WwfMgxjPRvCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the clusters\n",
        "model.visualize_topics()"
      ],
      "metadata": {
        "id": "GexsTvlNR6i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### myDoc2vec"
      ],
      "metadata": {
        "id": "gaQuSC3l8nTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown If True the word2vect model is trained using the corpus, otherwise a pre-trained model is downloaded:\n",
        "train = True    # @param {type: \"boolean\"}\n",
        "# @markdown The norm that must be used:\n",
        "norm = \"l2\" # @param [\"l1\",\"l2\"]\n",
        "# @markdown Word vector size:\n",
        "wv_size = 100    # @param {type: \"number\"}\n",
        "# @markdown Skip tokens that do not have a relative document frequency in this range:\n",
        "min_df = 0.02  # @param {type: \"number\"}\n",
        "max_df = 1.00  # @param {type: \"number\"}\n",
        "# @markdown ---\n",
        "\n",
        "# compute / download word vectors\n",
        "word2vector :np.ndarray\n",
        "if train:\n",
        "    model = gensim.models.Word2Vec(tokenized_corpus, vector_size=wv_size)\n",
        "    word2vector = model.wv\n",
        "else:\n",
        "    word2vector = gensim.downloader.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# vectorizer used to compute the tf-idf score\n",
        "tfidf_vectorizer =  TfidfVectorizer(tokenizer=identity_tokenizer, token_pattern=None, lowercase=False, norm=norm, min_df=min_df, max_df=max_df).fit(tokenized_corpus)\n",
        "\n",
        "# define the vocabolary shared by the word2vect model and the tf-idf vectorizer\n",
        "count = 0\n",
        "voc = set(tfidf_vectorizer.get_feature_names_out())\n",
        "for tk in tfidf_vectorizer.vocabulary_.keys():\n",
        "    if tk not in word2vector.key_to_index:\n",
        "        voc.remove(tk)\n",
        "        count += 1\n",
        "print(f\"Tokens removed: {count}\")"
      ],
      "metadata": {
        "id": "znBQ76UeECaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2v = myDoc2vec(\n",
        "    word2vector,\n",
        "    TfidfVectorizer(tokenizer=identity_tokenizer, token_pattern=None, lowercase=False, norm=norm, vocabulary=voc),\n",
        "    exp_a = 2.0,\n",
        "    exp_b = 3.0,\n",
        "    eps = 0.01\n",
        ")\n",
        "\n",
        "vec_corpus = d2v.transform(tokenized_corpus, save=True)"
      ],
      "metadata": {
        "id": "-GbaMzrREe3S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random example\n",
        "idx = np.random.randint(0, len(tokenized_corpus))\n",
        "\n",
        "print(f\"Random exemple [iloc:{idx}, loc:{corpus.index[idx]}] =>\")\n",
        "print(f\"{corpus.iloc[idx, 6]}\")\n",
        "print(\"------------------------------------------\")\n",
        "print(f\" - tokenized document: {tokenized_corpus[idx]}\")\n",
        "print(f\" - token weights: {d2v._weights[idx]}\")\n",
        "print(f\" - most similar: {d2v.w2v.most_similar(vec_corpus[idx], topn=5)}\")"
      ],
      "metadata": {
        "id": "gCfbZFnsrSGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K-Means"
      ],
      "metadata": {
        "id": "cIMnr24rrvQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use elbow method and silhouette score to estimate the number of clusters\n",
        "kmeans = KMeansHelper()\n",
        "\n",
        "_ = kmeans.elbow_method(vec_corpus, 2, 41)\n",
        "_ = kmeans.silhouette_score(vec_corpus, 2, 41)"
      ],
      "metadata": {
        "id": "DditmQKAr1fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of clusters\n",
        "K = 25   # PARAM\n",
        "# Number of most similar tokens to show per cluster\n",
        "W = 5   # PARAM\n",
        "\n",
        "kmeans = KMeansHelper(n_init=100, max_iter=10000)\n",
        "clusters, centroids = kmeans.fit_predict(vec_corpus, K)\n",
        "\n",
        "# print cluster found\n",
        "for k in range(K):\n",
        "    idx_docs_in_cluster = np.flatnonzero(clusters == k)\n",
        "\n",
        "    dists = 1 - d2v.w2v.cosine_similarities(centroids[k], vec_corpus[idx_docs_in_cluster])\n",
        "    min_dist, min_idx = np.nanmin(dists), idx_docs_in_cluster[np.nanargmin(dists)]\n",
        "    max_dist, max_idx = np.nanmax(dists), idx_docs_in_cluster[np.nanargmax(dists)]\n",
        "\n",
        "    print(f\"Cluster {k} =>\")\n",
        "    print(f\" - size: {len(idx_docs_in_cluster)}\")\n",
        "    print(f\" - closest tokens: {d2v.w2v.most_similar(centroids[k], topn=W)}\")\n",
        "    print(f\" - closest doc ({round(min_dist,4)}): {min_idx}\")\n",
        "    print(f\" - farthest doc ({round(max_dist,4)}): {max_idx}\")\n",
        "    print(f\" - random samples: {np.random.choice(idx_docs_in_cluster, 10)}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "y9BDMa9RxMc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documents and centroids as points in a n-dimensional space\n",
        "points_nD = np.concatenate((centroids, vec_corpus))\n",
        "\n",
        "# t-SNE to reduce the number of dimensions\n",
        "tsne = TSNE(n_components=2, max_iter=2000, perplexity=50)\n",
        "points_2D = tsne.fit_transform(points_nD)\n",
        "\n",
        "# plot the clusters with their centroids\n",
        "fig, ax = plt.subplots()\n",
        "scatter_docs = ax.scatter(points_2D[K:, 0], points_2D[K:, 1], c=clusters, alpha=0.5)\n",
        "scatter_cens = ax.scatter(points_2D[:K, 0], points_2D[:K, 1], c=[\"red\"], alpha=1.0, marker='^', label=\"Centroids\")\n",
        "legend_docs = ax.legend(*scatter_docs.legend_elements(), loc=\"lower right\", title=\"Clusters\")\n",
        "legend_cens = ax.legend(handles=[scatter_cens], loc=\"upper left\")\n",
        "ax.add_artist(legend_docs)\n",
        "ax.add_artist(legend_cens)\n",
        "plt.title(\"t-SNE: K-Means with myDoc2Vec vectors\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gLBRjfG0KZ8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subject's ..."
      ],
      "metadata": {
        "id": "12eeFNEvWvYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Corpus"
      ],
      "metadata": {
        "id": "LYuhiWCwZH2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Minimum and maximum length (in characters) of email bodies to be analyzed:\n",
        "min_len_cnt =  700  # @param {type: \"number\"}\n",
        "max_len_cnt = 1400  # @param {type: \"number\"}\n",
        "# @markdown Minimum and maximum length (in characters) of email subjects to be analyzed:\n",
        "min_len_sbj =   30  # @param {type: \"number\"}\n",
        "max_len_sbj =   70  # @param {type: \"number\"}\n",
        "# @markdown ---\n",
        "\n",
        "cnt_corpus = df[df[\"content\"].apply(lambda x: len(x)).between(min_len_cnt, max_len_cnt)]\n",
        "sbj_corpus = df[df[\"subject\"].apply(lambda x: len(x)).between(min_len_sbj, max_len_sbj)]"
      ],
      "metadata": {
        "id": "VnJqdR-YCmt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "cYkUiKJrZMSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp = Preprocessor(\n",
        "    case_sensitive = False,\n",
        "    lemmatize = True,\n",
        "    noun_chunks = False,\n",
        "    keep_stopwords = False,\n",
        "    extend_stopwords = {\"email\", \"fw\", \"mail\", \"hereto\", \"link\", \"pm\", \"re\", \"recipient\", \"sender\", \"thank\", \"thanks\", \"thereto\", \"time\"},\n",
        "    regex_flags = re.IGNORECASE,\n",
        "    regex_invalid_line = Preprocessor.CREIL.HEADER,\n",
        "    regex_invalid_tokens = Preprocessor.CREIT.ALPHA_2,\n",
        "    pos_to_keep = {\"ADJ\", \"NOUN\", \"VERB\"},\n",
        "    ent_to_rm = {\"DATE\", \"TIME\"}\n",
        ")\n",
        "\n",
        "tokenized_cnt_corpus = list(cnt_corpus[\"content\"].progress_apply(pp.preprocess))\n",
        "tokenized_sbj_corpus = list(sbj_corpus[\"subject\"].progress_apply(pp.preprocess))\n",
        "\n",
        "# create a single corpus containing both email bodies and objects\n",
        "tokenized_corpus = tokenized_cnt_corpus +tokenized_sbj_corpus"
      ],
      "metadata": {
        "id": "9etJxDONBPcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### myDoc2vec"
      ],
      "metadata": {
        "id": "YgjO6arkZSw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown If True the word2vect model is trained using the corpus, otherwise a pre-trained model is downloaded:\n",
        "train = True    # @param {type: \"boolean\"}\n",
        "# @markdown The norm that must be used:\n",
        "norm = \"l1\" # @param [\"l1\",\"l2\"]\n",
        "# @markdown Word vector size:\n",
        "wv_size = 50    # @param {type: \"number\"}\n",
        "# @markdown Skip tokens that do not have a relative document frequency in this range:\n",
        "min_df = 0.02  # @param {type: \"number\"}\n",
        "max_df = 1.00  # @param {type: \"number\"}\n",
        "# @markdown ---\n",
        "\n",
        "# compute / download word vectors\n",
        "word2vector :np.ndarray\n",
        "if train:\n",
        "    model = gensim.models.Word2Vec(tokenized_corpus, vector_size=wv_size)\n",
        "    word2vector = model.wv\n",
        "else:\n",
        "    word2vector = gensim.downloader.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# vectorizer used to compute the tf-idf score\n",
        "tfidf_vectorizer =  TfidfVectorizer(tokenizer=identity_tokenizer, token_pattern=None, lowercase=False, norm=norm, min_df=min_df, max_df=max_df).fit(tokenized_corpus)\n",
        "\n",
        "# define the vocabolary shared by the word2vect model and the tf-idf vectorizer\n",
        "count = 0\n",
        "voc = set(tfidf_vectorizer.get_feature_names_out())\n",
        "for tk in tfidf_vectorizer.vocabulary_.keys():\n",
        "    if tk not in word2vector.key_to_index:\n",
        "        voc.remove(tk)\n",
        "        count += 1\n",
        "print(f\"Tokens removed: {count}\")"
      ],
      "metadata": {
        "id": "BDAUF13MIykm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2v = myDoc2vec(\n",
        "    word2vector,\n",
        "    TfidfVectorizer(tokenizer=identity_tokenizer, token_pattern=None, lowercase=False, norm=norm, vocabulary=voc),\n",
        "    exp_a = 2.0,\n",
        "    exp_b = 1.0,\n",
        "    eps = 0.02\n",
        ")\n",
        "\n",
        "vec_corpus = d2v.transform(tokenized_sbj_corpus)"
      ],
      "metadata": {
        "id": "ASAifD29I11a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random example\n",
        "idx = np.random.randint(0, len(tokenized_sbj_corpus))\n",
        "\n",
        "print(f\"Random exemple [iloc:{idx}, loc:{sbj_corpus.index[idx]}] =>\")\n",
        "print(f\"{sbj_corpus.iloc[idx, 6]}\")\n",
        "print(\"------------------------------------------\")\n",
        "print(f\" - tokenized document: {tokenized_sbj_corpus[idx]}\")\n",
        "print(f\" - token weights: {d2v.get_weights(tokenized_sbj_corpus)[idx]}\")\n",
        "print(f\" - most similar: {d2v.w2v.most_similar(vec_corpus[idx], topn=5)}\")"
      ],
      "metadata": {
        "id": "6Fs2mnzBQn_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Means with myDoc2vec"
      ],
      "metadata": {
        "id": "wLk_G-aXZVFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use elbow method and silhouette score to estimate the number of clusters\n",
        "kmeans = KMeansHelper()\n",
        "\n",
        "_ = kmeans.elbow_method(vec_corpus, 2, 12)\n",
        "_ = kmeans.silhouette_score(vec_corpus, 2, 12)"
      ],
      "metadata": {
        "id": "cTMUWmiARbkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of clusters\n",
        "K = 6   # PARAM\n",
        "# Number of most similar tokens to show per cluster\n",
        "W = 5   # PARAM\n",
        "\n",
        "kmeans = KMeansHelper()\n",
        "clusters, centroids = kmeans.fit_predict(vec_corpus, K)\n",
        "\n",
        "# print cluster found\n",
        "for k in range(K):\n",
        "    idx_docs_in_cluster = np.flatnonzero(clusters == k)\n",
        "\n",
        "    dists = 1 - d2v.w2v.cosine_similarities(centroids[k], vec_corpus[idx_docs_in_cluster])\n",
        "    min_dist, min_idx = np.nanmin(dists), idx_docs_in_cluster[np.nanargmin(dists)]\n",
        "    max_dist, max_idx = np.nanmax(dists), idx_docs_in_cluster[np.nanargmax(dists)]\n",
        "\n",
        "    print(f\"Cluster {k} =>\")\n",
        "    print(f\" - size: {len(idx_docs_in_cluster)}\")\n",
        "    print(f\" - closest tokens: {d2v.w2v.most_similar(centroids[k], topn=W)}\")\n",
        "    print(f\" - closest doc ({round(min_dist,4)}): {min_idx}\")\n",
        "    print(f\" - farthest doc ({round(max_dist,4)}): {max_idx}\")\n",
        "    print(f\" - random samples: {np.random.choice(idx_docs_in_cluster, 10)}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "vN6ZQgh6ZBOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documents and centroids as points in a n-dimensional space\n",
        "points_nD = np.concatenate((centroids, vec_corpus))\n",
        "\n",
        "# t-SNE to reduce the number of dimensions\n",
        "tsne = TSNE(n_components=2, max_iter=1000, perplexity=400)\n",
        "points_2D = tsne.fit_transform(points_nD)\n",
        "\n",
        "# plot the clusters with their centroids\n",
        "fig, ax = plt.subplots()\n",
        "scatter_docs = ax.scatter(points_2D[K:, 0], points_2D[K:, 1], c=clusters, alpha=0.5)\n",
        "scatter_cens = ax.scatter(points_2D[:K, 0], points_2D[:K, 1], c=[\"red\"], alpha=1.0, marker='^', label=\"Centroids\")\n",
        "legend_docs = ax.legend(*scatter_docs.legend_elements(), loc=\"lower right\", title=\"Clusters\")\n",
        "legend_cens = ax.legend(handles=[scatter_cens], loc=\"upper left\")\n",
        "ax.add_artist(legend_docs)\n",
        "ax.add_artist(legend_cens)\n",
        "plt.title(\"t-SNE: K-Means with myDoc2Vec vectors\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-7WLESV7Y31Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra"
      ],
      "metadata": {
        "id": "fjLuBTyjkBaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the tokenized corpus for future use\n",
        "import pickle\n",
        "\n",
        "def write_var(variable, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(variable, f)\n",
        "\n",
        "def read_var(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        variable = pickle.load(f)\n",
        "    return variable\n",
        "\n",
        "#write_var(vec_corpus, \"vec_corpus_2.pkl\")\n",
        "#tokenized_corpus = read_var(\"tokenized_corpus.pkl\")"
      ],
      "metadata": {
        "id": "f_eBWBeVuM4y"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}