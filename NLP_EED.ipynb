{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPU+U3Zb59RGZvdrXTwgnA/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoOnger/NLP_Project/blob/main/NLP_EED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Project: Enron Email Dataset**\n",
        "\n",
        "*   **Author:** Matteo Onger\n",
        "*   **Date:** December 2024"
      ],
      "metadata": {
        "id": "XvcVpru0tpu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset documentation**:\n",
        "*   [Enron email dataset](https://www.kaggle.com/datasets/wcukierski/enron-email-dataset)\n",
        "\n",
        "**Extra documentation**:\n",
        "*   [POS](https://universaldependencies.org/u/pos/)\n",
        "*   [SpaCy - English](https://spacy.io/models/en#en_core_web_sm)"
      ],
      "metadata": {
        "id": "OqwGP1zJtujv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- LIBRARIES ----\n",
        "import email\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "from enum import Enum\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import silhouette_score\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.language import Language\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "# init spacy\n",
        "flag_gpu = spacy.prefer_gpu()\n",
        "print(f\"GPU available:{flag_gpu}\")\n",
        "\n",
        "\n",
        "# init tpqdm\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "WSNfJzTedW2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- FUNCTIONS ----\n",
        "def extract_fields(msg :email.message, fields :list[str]|None=None) -> dict[str, str]:\n",
        "    \"\"\"\n",
        "    This function extracts the required fields from the given email message.\n",
        "    If no field is not specified, all the available ones are returned.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    msg : email.message\n",
        "        Email message.\n",
        "    fields : list | None, optional\n",
        "        The fields to extract, by default ``None``.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    :dict[str, str]\n",
        "        A dictionary ``{field_name:field_value}`` containing the fields extracted is returned.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - If  ``fields`` is ``None``, all the available fields are extracted.\n",
        "    - Fields must be in lower case.\n",
        "    \"\"\"\n",
        "    res = dict()\n",
        "\n",
        "    if fields is None:\n",
        "        res = {str.lower(key):msg[key] for key in msg.keys()}\n",
        "    else:\n",
        "        res = {str.lower(key):msg[key] for key in msg.keys() if str.lower(key) in fields}\n",
        "\n",
        "    if fields is None or \"content\" in fields:\n",
        "        parts = list()\n",
        "        for part in msg.walk():\n",
        "            if part.get_content_type() == 'text/plain':\n",
        "                parts.append(part.get_payload())\n",
        "        res[\"content\"] = \"\".join(parts)\n",
        "    return res\n",
        "\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "    \"\"\"\n",
        "    Returns ``text`` as it is.\n",
        "    \"\"\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def split_email_addresses(addresses :str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Extracts the email addresses contained in the provided string.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    addresses : str\n",
        "        String containing email addresses.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    :list[str]\n",
        "        List of strings that are the email addresses found.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - If ``addresses`` is not a string, an empty list is returned.\n",
        "    \"\"\"\n",
        "    if isinstance(addresses, str):\n",
        "        addrs = re.findall(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', addresses)\n",
        "    else:\n",
        "        addrs = list()\n",
        "    return addrs"
      ],
      "metadata": {
        "id": "cGFhHQYDGqS2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- CLASSES ----\n",
        "class Preprocessor():\n",
        "    \"\"\"\n",
        "    This class implements a simple preprocessor to split documents into lists of tokens.\n",
        "    It is mainly based on spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    class CREIL(Enum):\n",
        "        \"\"\"\n",
        "        Common regex of invalid lines.\n",
        "        \"\"\"\n",
        "        HEADER = r\"^\\s*(?:-{5,}|.*-{5,}|from:|to:|cc:|ccn:|sent by:|subject:).*$\"\n",
        "        \"\"\"\n",
        "        Removes (forwarded, etc.) email header and blank lines.\n",
        "        \"\"\"\n",
        "\n",
        "    class CREIT(Enum):\n",
        "        \"\"\"\n",
        "        Common regex of invalid tokens.\n",
        "        \"\"\"\n",
        "        ALPHA_2 = r\"^(?:.*[^a-z_]{1,}.*|.)$\"\n",
        "        \"\"\"\n",
        "        Only tokens of two or more characters consisting only of alphabetic characters are retained.\n",
        "        \"\"\"\n",
        "        ALPHANUM = r\".*(?:\\W|_).*\"\n",
        "        \"\"\"\n",
        "        Removes tokens that contain non-alphanumeric characters.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    @Language.component(\"lower_case_lemmas\")\n",
        "    def _lower_case_lemmas(doc :spacy.tokens.Doc) -> spacy.tokens.Doc:\n",
        "        \"\"\"\n",
        "        Changes the capitalization of the lemmas to lowercase.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        doc : spacy.tokens.Doc\n",
        "            Doc to modify.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :spacy.tokens.Doc\n",
        "            Doc modified.\n",
        "        \"\"\"\n",
        "        for token in doc :\n",
        "            token.lemma_ = token.lemma_.lower()\n",
        "        return doc\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        case_sensitive :bool=True,\n",
        "        lemmatize :bool=False,\n",
        "        noun_chunks :bool=False,\n",
        "        keep_stopwords :bool=True,\n",
        "        extend_stopwords :set[str]=None,\n",
        "        regex_flags :int=0,\n",
        "        regex_invalid_line :'str|Preprocessor.CREIL'=None,\n",
        "        regex_invalid_tokens :'str|Preprocessor.CREIT'=None,\n",
        "        ent_to_keep :set[str]=None,\n",
        "        ent_to_rm :set[str]=None,\n",
        "        pos_to_keep :set[str]=None,\n",
        "        pos_to_rm :set[str]=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        case_sensitive : bool, optional\n",
        "            If ``False``, all returned tokens are lowercase, by default ``True``.\n",
        "        lemmatize : bool, optional\n",
        "            If ``True``, tokens are lemmatized, by default ``False``.\n",
        "        noun_chunks : bool, optional\n",
        "            If ``True``, chunk nouns are merged, by default ``False``.\n",
        "        keep_stopwords : bool, optional\n",
        "            If ``True``, stop words are kept, by default ``False``.\n",
        "        extend_stopwords : set[str], optional\n",
        "            List of stop words to add, by default ``None``.\n",
        "        regex_flags : int, optional\n",
        "            Flags of the regular expressions, by default ``0``.\n",
        "        regex_invalid_line : str|Preprocessor.CREIL, optional\n",
        "            Regex to mark lines that must be excluded, by default ``None``.\n",
        "        regex_invalid_tokens : str|Preprocessor.CREIT, optional\n",
        "            Regex to mark tokens that must to be excluded, by default ``None``.\n",
        "        ent_to_keep : set[str], optional\n",
        "            Only tokens marked with one of these entity tags are kept,\n",
        "            by default all tokens are retained.\n",
        "        ent_to_rm : set[str], optional\n",
        "            Tokens marked with one of these entity tags are removed,\n",
        "            by default all tokens are retained.\n",
        "        pos_to_keep : set[str], optional\n",
        "            Only tokens marked with one of these POS tags are kept,\n",
        "            by default all tokens are retained.\n",
        "        pos_to_rm : set[str], optional\n",
        "            Tokens marked with one of these POS tags are removed,\n",
        "            by default all tokens are retained.\n",
        "        \"\"\"\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "        # update nlp piepline\n",
        "        if not lemmatize and not noun_chunks:\n",
        "            self.nlp.remove_pipe(\"lemmatizer\")\n",
        "            if ent_to_keep is None and ent_to_rm is None and pos_to_keep is None and pos_to_rm is None:\n",
        "                self.nlp.remove_pipe(\"tagger\")\n",
        "                self.nlp.remove_pipe(\"parser\")\n",
        "                self.nlp.remove_pipe(\"attribute_ruler\")\n",
        "                self.nlp.remove_pipe(\"ner\")\n",
        "        if noun_chunks:\n",
        "            self.nlp.add_pipe(\"merge_noun_chunks\")\n",
        "        if lemmatize and not case_sensitive:\n",
        "            self.nlp.add_pipe(\"lower_case_lemmas\", name=\"lower_case_lemmas\")\n",
        "\n",
        "        # standard regex\n",
        "        self.regex_empty_line = r\"^\\s*$\"\n",
        "        self.regex_endline_1 = r\"(=20\\n)\"\n",
        "        self.regex_endline_2 = r\"(=\\n)\"\n",
        "\n",
        "        self.pattern_empty_line = re.compile(self.regex_empty_line, regex_flags)\n",
        "        self.pattern_endline_1 = re.compile(self.regex_endline_1, regex_flags)\n",
        "        self.pattern_endline_2 = re.compile(self.regex_endline_2, regex_flags)\n",
        "\n",
        "        # save fields\n",
        "        self.case_sensitive = case_sensitive\n",
        "        self.noun_chunks = noun_chunks\n",
        "        self.lemmatize = lemmatize\n",
        "        self.keep_stopwords = keep_stopwords\n",
        "        self.extend_stopwords = extend_stopwords\n",
        "        self.regex_flags = regex_flags\n",
        "        self.regex_invalid_line = regex_invalid_line.value if isinstance(regex_invalid_line, Preprocessor.CREIL) else regex_invalid_line\n",
        "        self.regex_invalid_tokens = regex_invalid_tokens.value if isinstance(regex_invalid_tokens,  Preprocessor.CREIT) else regex_invalid_tokens\n",
        "        self.ent_to_keep = ent_to_keep\n",
        "        self.ent_to_rm = ent_to_rm\n",
        "        self.pos_to_keep = pos_to_keep\n",
        "        self.pos_to_rm = pos_to_rm\n",
        "\n",
        "        # assemble the condition that tokens/lines must satisfy\n",
        "        self.conditions = list()\n",
        "        if not keep_stopwords:\n",
        "            self.conditions.append(lambda x: not x.is_stop)\n",
        "            if extend_stopwords is not None:\n",
        "                if lemmatize:\n",
        "                    self.conditions.append(lambda x: x.lemma_ not in extend_stopwords)\n",
        "                else:\n",
        "                    self.conditions.append(lambda x: x.text not in extend_stopwords if case_sensitive else x.text.lower() not in extend_stopwords)\n",
        "        if ent_to_keep is not None:\n",
        "            self.conditions.append(lambda x: x.ent_type_ in ent_to_keep)\n",
        "        if ent_to_rm is not None:\n",
        "            self.conditions.append(lambda x: x.ent_type_ not in ent_to_rm)\n",
        "        if pos_to_keep is not None:\n",
        "            self.conditions.append(lambda x: x.pos_ in pos_to_keep)\n",
        "        if pos_to_rm is not None:\n",
        "            self.conditions.append(lambda x: x.pos_ in pos_to_rm)\n",
        "\n",
        "        if regex_invalid_line is not None:\n",
        "            self.pattern_invalid_line = re.compile(self.regex_invalid_line, regex_flags)\n",
        "        if regex_invalid_tokens is not None:\n",
        "            self.pattern_invalid_tokens = re.compile(self.regex_invalid_tokens, regex_flags)\n",
        "            self.conditions.append(lambda x: not self.pattern_invalid_tokens.match(x.text))\n",
        "        return\n",
        "\n",
        "\n",
        "    def preprocess(self, document :str) -> list[str]:\n",
        "        \"\"\"\n",
        "        Applies the preprocessing procedure to the given document.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        document : str\n",
        "            Document to process.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :list[str]\n",
        "            List of tokens representing the document.\n",
        "        \"\"\"\n",
        "        # MIME protocol for base64, the max line length in the encoded data is 76 characters\n",
        "        # an '=' sign at the end of an encoded line is used to tell the decoder that the line is to be continued,\n",
        "        # while an '=20' sign is used to tell the line ends\n",
        "        document = self.pattern_endline_1.sub(\"\\n\", document)\n",
        "        document = self.pattern_endline_2.sub(\"\", document)\n",
        "\n",
        "        # read the document line by line and remove the invalid ones that are:\n",
        "        # - blank lines\n",
        "        # - lines that match the given regex <self.regex_invalid_line>\n",
        "        # - lines preceded by invalid non-white lines\n",
        "        prev = True\n",
        "        filtered_document = list()\n",
        "        for line in document.splitlines():\n",
        "            if self.pattern_empty_line.match(line):\n",
        "                prev = True\n",
        "            elif prev and (self.regex_invalid_line is None or not self.pattern_invalid_line.match(line)):\n",
        "                filtered_document.append(line)\n",
        "            else:\n",
        "                prev = False\n",
        "\n",
        "        # replace all withespaces with a single whitespace\n",
        "        document = re.sub(r\"\\s{1,}\", \" \", \"\\n\".join(filtered_document))\n",
        "\n",
        "        # tokenize, etc. using spaCy\n",
        "        doc = self.nlp(document)\n",
        "        if self.lemmatize:\n",
        "            tokens = [tk.lemma_ for tk in doc if all(f(tk) for f in self.conditions)]\n",
        "        else:\n",
        "            if self.case_sensitive:\n",
        "                tokens = [tk.text for tk in doc if all(f(tk) for f in self.conditions)]\n",
        "            else:\n",
        "                tokens = [tk.text.lower() for tk in doc if all(f(tk) for f in self.conditions)]\n",
        "        return tokens\n",
        "\n",
        "\n",
        "\n",
        "class KMeansHelper():\n",
        "    \"\"\"\n",
        "    This class is based on the module ``sklearn.cluster.KMeans`` and\n",
        "    provides some heuristicher to estimate the best number of clusters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        init :str=\"k-means++\",\n",
        "        n_init :int|str=\"auto\",\n",
        "        random_state :int=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        init : str, optional\n",
        "            Method for initialization, by default ``'k-means++'``:\n",
        "            - ``'k-means++'``: selects initial cluster centroids using sampling based on an empirical probability distribution\n",
        "            of the points' contribution to the overall inertia. This technique speeds up convergence.\n",
        "            The algorithm implemented is greedy k-means++. It differs from the vanilla k-means++ by making several trials\n",
        "            at each sampling step and choosing the best centroid among them.\n",
        "            - ``'random'``: choose n_clusters observations at random from data for the initial centroids.\n",
        "        n_init : str, optional\n",
        "            Number of times the k-means algorithm is run with different centroid seeds.\n",
        "            By default ``n_init='auto'``, the number of runs depends on the value of init:\n",
        "            10 if using ``init='random'``; 1 if using ``init='k-means++'``.\n",
        "        random_state : int, optional\n",
        "            Determines random number generation for centroid initialization. Use an int to make the randomness deterministic,\n",
        "            by default ``None``.\n",
        "        \"\"\"\n",
        "        self.init = init\n",
        "        self.n_init = n_init\n",
        "        self.random_state = random_state\n",
        "        return\n",
        "\n",
        "\n",
        "    def elbow_method(self, X :np.ndarray, min_K :int, max_K :int) -> dict[int, int]:\n",
        "        \"\"\"\n",
        "        Returns and draws the inertia as the number of clusters changes.\n",
        "        This heuristic is used to determine the number of clusters in a data set ``X``.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray\n",
        "            New data to transform.\n",
        "        min_K : int\n",
        "            Minimum number of clusters.\n",
        "        max_K : int\n",
        "            maximum number of clusters.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :dict[int, int]\n",
        "            A dictionary containing the SSE for every possible number of clusters\n",
        "            between ``min_K`` and ``max_K``.\n",
        "        \"\"\"\n",
        "        sse = {}\n",
        "        for k in range(min_K, max_K):\n",
        "            kmeans = KMeans(n_clusters=k, init=self.init, n_init=self.n_init, random_state=self.random_state).fit(X)\n",
        "            sse[k] = kmeans.inertia_\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(list(sse.keys()), list(sse.values()), marker='*')\n",
        "        plt.title(\"Elbow method\")\n",
        "        plt.xlabel(\"Number of cluster (K)\")\n",
        "        plt.ylabel(\"SSE\")\n",
        "        plt.show()\n",
        "        return sse\n",
        "\n",
        "\n",
        "    def fit_predict(self, X :np.ndarray, K :int) -> tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Compute cluster centers and predict cluster index for each sample.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray\n",
        "            New data to transform.\n",
        "        K : int\n",
        "            Number of clusters.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :tuple[np.ndarray, np.ndarray]\n",
        "            Index of the cluster each sample belongs to and the centroids\n",
        "        \"\"\"\n",
        "        kmeans = KMeans(n_clusters=K, init=self.init, n_init=self.n_init, random_state=self.random_state)\n",
        "        clusters = kmeans.fit_predict(X)\n",
        "        centroids = kmeans.cluster_centers_\n",
        "        return clusters, centroids\n",
        "\n",
        "\n",
        "    def silhouette_score(self, X :np.ndarray, min_K :int, max_K :int) -> dict[int, int]:\n",
        "        \"\"\"\n",
        "        Returns the silhouette score [-1, 1] as the number of clusters changes.\n",
        "        This heuristic is used to determine the number of clusters in a data set ``X``.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray\n",
        "            New data to transform.\n",
        "        min_K : int\n",
        "            Minimum number of clusters.\n",
        "        max_K : int\n",
        "            maximum number of clusters.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :dict[int, int]\n",
        "            A dictionary containing the silhouette score for every possible number of clusters\n",
        "            between ``min_K`` and ``max_K``.\n",
        "        \"\"\"\n",
        "        sc = {}\n",
        "        print(\"Number of clusters:\")\n",
        "        for k in range(min_K, max_K):\n",
        "            kmeans = KMeans(n_clusters=k, init=self.init, n_init=self.n_init, random_state=self.random_state).fit(X)\n",
        "            sc[k] = silhouette_score(X, kmeans.labels_, metric='euclidean')\n",
        "            print(f\" - K:{k} => Silhouette coeff.: {sc[k]}\")\n",
        "        return sc"
      ],
      "metadata": {
        "id": "k-1hpTHhOCtS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus"
      ],
      "metadata": {
        "id": "yuogXTnXI-WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- DATASET ----\n",
        "# download latest version\n",
        "path = kagglehub.dataset_download(\"wcukierski/enron-email-dataset\")\n",
        "print(f\"Path to dataset files: {path}\")\n",
        "\n",
        "# read the dataset\n",
        "df = pd.read_csv(path + \"/emails.csv\")\n",
        "\n",
        "# from string to email.message\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: email.message_from_string(x))\n",
        "\n",
        "# extract main fields\n",
        "df = df.join(\n",
        "    df.apply(\n",
        "        lambda row: extract_fields(row.message, [\"message-id\", \"date\", \"from\", \"to\", \"subject\", \"content\"]),\n",
        "        axis='columns',\n",
        "        result_type='expand'\n",
        "    )\n",
        ")\n",
        "\n",
        "# drop unused column and duplicated messages\n",
        "df.drop(columns=[\"message\"], inplace=True)\n",
        "df.drop_duplicates([\"content\"], keep=\"first\", inplace=True)\n",
        "\n",
        "# split addresses into a list of strings\n",
        "df[\"from\"] = df[\"from\"].apply(\n",
        "    lambda x: split_email_addresses(x),\n",
        ")\n",
        "df[\"to\"] = df[\"to\"].apply(\n",
        "    lambda x: split_email_addresses(x),\n",
        ")"
      ],
      "metadata": {
        "id": "_SOiq5ihtsXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# minimum and maximum length (in characters) of emails to be analyzed\n",
        "min_len, max_len = 600, 610\n",
        "# percentage (0, 1] of emails that must be analyzed\n",
        "sample_frac = 1.\n",
        "\n",
        "corpus = df[df[\"content\"].apply(lambda x: len(x)).between(min_len, max_len)].sample(frac=sample_frac)\n",
        "\n",
        "display(corpus)\n",
        "print(f\"\\nRandom exemple =>\\n{corpus.sample(1).iloc[0, 6]}\")"
      ],
      "metadata": {
        "id": "i9pHmXixNa88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "0T6-0idLJJSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp = Preprocessor(\n",
        "    case_sensitive = False,\n",
        "    lemmatize = True,\n",
        "    noun_chunks = False,\n",
        "    keep_stopwords = False,\n",
        "    extend_stopwords = {\"email\", \"mail\", \"hereto\", \"link\", \"pm\", \"recipient\", \"sender\", \"thank\", \"thanks\", \"time\"},\n",
        "    regex_flags = re.IGNORECASE,\n",
        "    regex_invalid_line = Preprocessor.CREIL.HEADER,\n",
        "    regex_invalid_tokens = Preprocessor.CREIT.ALPHA_2,\n",
        "    pos_to_keep = {\"NOUN\"},\n",
        "    ent_to_rm = {\"DATE\", \"TIME\"}\n",
        ")\n",
        "\n",
        "tokenized_corpus = list(corpus[\"content\"].progress_apply(pp.preprocess))"
      ],
      "metadata": {
        "id": "xvyP3_QzXeu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF"
      ],
      "metadata": {
        "id": "eoTZ_C3yIyiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# skip tokens that do not have a relative document frequency in this range\n",
        "min_df = 0.01\n",
        "max_df = 1.00\n",
        "\n",
        "tfidf_vectorizer =  TfidfVectorizer(tokenizer=identity_tokenizer, token_pattern=None, lowercase=False, min_df=min_df, max_df=max_df)\n",
        "tfidf_corpus = tfidf_vectorizer.fit_transform(tokenized_corpus)\n",
        "\n",
        "tfidf_df = pd.DataFrame(tfidf_corpus.toarray(), index=corpus.index, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "display(tfidf_df)"
      ],
      "metadata": {
        "id": "IXcnSURMXWA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KMeans with TF-IDF"
      ],
      "metadata": {
        "id": "XUr-LS1WJMJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use elbow method and silhouette score to estimate the number of clusters\n",
        "kmeans = KMeansHelper()\n",
        "\n",
        "_ = kmeans.elbow_method(tfidf_corpus, 2, 12)\n",
        "_ = kmeans.silhouette_score(tfidf_corpus, 2, 12)"
      ],
      "metadata": {
        "id": "R-50ewHJ0RTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of clusters\n",
        "K = 6\n",
        "# number of tokens with the heighest tf-idf score to show per cluster\n",
        "W = 5\n",
        "\n",
        "clusters, centroids = kmeans.fit_predict(tfidf_corpus, K)\n",
        "\n",
        "# for each cluster, sort the token indexes according to their tf-idf score (descending order)\n",
        "argsort_tokens = (np.argsort(centroids, axis=1)[:, ::-1])\n",
        "# find the index of W tokens with the highest tf-idf score that are not among\n",
        "# W tokens with the highest tf-idf score in other clusters\n",
        "unique_main_tk_idxs = np.full((K, W), -1)\n",
        "\n",
        "for k in range(K):\n",
        "    counter = 0\n",
        "    for tk_idx in argsort_tokens[k]:\n",
        "        if tk_idx not in np.delete(argsort_tokens[:, :W], k, axis=0) and tk_idx not in unique_main_tk_idxs:\n",
        "            unique_main_tk_idxs[k, counter] = tk_idx\n",
        "            counter += 1\n",
        "        if counter == W:\n",
        "            break\n",
        "\n",
        "    main_tokens = tfidf_vectorizer.get_feature_names_out()[argsort_tokens[k, :W]]\n",
        "    unique_main_tokens = tfidf_vectorizer.get_feature_names_out()[unique_main_tk_idxs[k]]\n",
        "    idx_docs_in_cluster = np.flatnonzero(clusters == k)\n",
        "\n",
        "    print(f\"Cluster {k} =>\")\n",
        "    print(f\" - size: {len(idx_docs_in_cluster)}\")\n",
        "    print(f\" - main tokens: {main_tokens}\")\n",
        "    print(f\" - unique main tokens: {unique_main_tokens}\")\n",
        "    print(f\" - random samples: {np.random.choice(idx_docs_in_cluster, 3)}\")\n",
        "print()\n",
        "\n",
        "# documents and centroids as points in a n-dimensional space\n",
        "points_nD = np.concatenate((centroids, tfidf_corpus.toarray()))\n",
        "\n",
        "# t-SNE to reduce the number of dimensions\n",
        "tsne = TSNE(n_components=2, perplexity=25)\n",
        "points_2D = tsne.fit_transform(points_nD)\n",
        "\n",
        "# plot the clusters with their centroids\n",
        "fig, ax = plt.subplots()\n",
        "scatter_docs = ax.scatter(points_2D[K:, 0], points_2D[K:, 1], c=clusters, alpha=0.5)\n",
        "scatter_cens = ax.scatter(points_2D[:K, 0], points_2D[:K, 1], c=[\"red\"], alpha=1.0, marker='^', label=\"Centroids\")\n",
        "legend_docs = ax.legend(*scatter_docs.legend_elements(), loc=\"lower right\", title=\"Clusters\")\n",
        "legend_cens = ax.legend(handles=[scatter_cens], loc=\"upper left\")\n",
        "ax.add_artist(legend_docs)\n",
        "ax.add_artist(legend_cens)\n",
        "plt.title(\"t-SNE: KMeans with TF-IDF vectors\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T9j3BdMOOO3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vect"
      ],
      "metadata": {
        "id": "gaQuSC3l8nTw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tBxagOI_8pBH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}